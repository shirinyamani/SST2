{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae1e34da-5fff-4749-8fe7-4c9cea02d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24 in /home/shirin/anaconda3/envs/ali-ml/lib/python3.10/site-packages (1.24.0)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "#import numpy==1.24 as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import imodelsx\n",
    "import collections\n",
    "\n",
    "!pip install numpy==1.24  # Replace '1.24' with the specific version you want\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43567e-195c-4522-8651-d71eddd6a0c1",
   "metadata": {},
   "source": [
    "## Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8636d2e5-3004-45e2-84a7-e1dafc743e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "76bb6660-368d-4eda-8a7a-59740ff641e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26fe6a73-f891-4c81-a680-e022025f45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017986b7-cc11-47ef-a835-efb3a227f027",
   "metadata": {},
   "source": [
    "## Sample very first two reviews from train DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4813d24-354e-410f-aef0-17ac42713ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Review: hide new secretions from the parental units \n",
      "Second Review: contains no wit , only labored gags \n"
     ]
    }
   ],
   "source": [
    "# Extract the first two reviews\n",
    "rev1 = ds[\"train\"][\"sentence\"][0]\n",
    "rev2 = ds[\"train\"][\"sentence\"][1]\n",
    "\n",
    "# Print the reviews\n",
    "print(\"First Review:\", rev1)\n",
    "print(\"Second Review:\", rev2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3664d8-53d5-42d0-9ff9-4a9d8d6a4cd3",
   "metadata": {},
   "source": [
    "## Compute Embedding w/ simple ```bert-uncased```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3a4a208-4089-443a-abc0-4740dece1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the reviews\n",
    "tokenized_reviews = tokenizer([rev1, rev2], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_reviews)\n",
    "\n",
    "# Extract the embeddings for each review\n",
    "review1_embedding = outputs.last_hidden_state[0][0]\n",
    "review2_embedding = outputs.last_hidden_state[1][0]\n",
    "\n",
    "#print(review1_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4cc09fd-4997-42c1-b09f-cadbad22538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 Embedding Shape: torch.Size([11, 768])\n",
      "Sentence 2 Embedding Shape: torch.Size([11, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define your sample sentences\n",
    "sentences = [rev1, rev2]\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    # Pass the tokenized input through the model\n",
    "    outputs = model(**tokenized_sentences)\n",
    "\n",
    "# Extract the embeddings for each sentence\n",
    "# Here, we assume that the number of sentences is the same as the number of rows in the input\n",
    "# and that you want to extract the embeddings for each sentence separately\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Now, you have the embeddings for each sentence in the `embeddings` tensor\n",
    "# You can access them like this:\n",
    "sentence1_embedding = embeddings[0]\n",
    "sentence2_embedding = embeddings[1]\n",
    "\n",
    "# Ensure the embeddings have the same shape\n",
    "print(\"Sentence 1 Embedding Shape:\", sentence1_embedding.shape)\n",
    "print(\"Sentence 2 Embedding Shape:\", sentence2_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415a5cb-95fd-4fe5-af3c-5d26ad08c82f",
   "metadata": {},
   "source": [
    "## Compute similarity w/ Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ad7eda0-b674-4a72-a0a2-12ad98f1c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.69258183\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "similarity_score = cosine_similarity(review1_embedding.reshape(1, -1), review2_embedding.reshape(1, -1))[0][0]\n",
    "\n",
    "print(\"Cosine Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f3a76-236f-4412-8ebd-fce99611bc8f",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71f77b01-41e4-43a5-ab53-5e216486ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract n-grams from a sentence\n",
    "def extract_ngrams(sentence, n):\n",
    "    tokens = sentence.split()\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "    return [' '.join(gram) for gram in ngrams_list]\n",
    "\n",
    "# Function to calculate n-gram importance\n",
    "def calculate_ngram_importance(rev1, rev2, n):\n",
    "    # Extract n-grams from both reviews\n",
    "    ngrams_review1 = extract_ngrams(rev1, n)\n",
    "    ngrams_review2 = extract_ngrams(rev2, n)\n",
    "    \n",
    "    # Count the frequency of each n-gram in both reviews\n",
    "    ngram_counts_review1 = Counter(ngrams_review1)\n",
    "    ngram_counts_review2 = Counter(ngrams_review2)\n",
    "    \n",
    "    # Calculate the intersection of n-grams\n",
    "    intersection_ngrams = set(ngram_counts_review1.keys()) & set(ngram_counts_review2.keys())\n",
    "    \n",
    "    # Calculate the importance of each common n-gram as the sum of their frequencies\n",
    "    ngram_importance = {ngram: ngram_counts_review1[ngram] + ngram_counts_review2[ngram] for ngram in intersection_ngrams}\n",
    "    \n",
    "    # Sort n-grams by importance (descending)\n",
    "    sorted_ngram_importance = dict(sorted(ngram_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_ngram_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c6acd-46b5-462d-bcbb-f711ef5db3c7",
   "metadata": {},
   "source": [
    "## Compare with lime/ Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e4b312a-767b-421a-8473-26fc8cebae17",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The model produced 2 output rows when given 1 input rows! Check the implementation of the model you provided for errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(predict_similarity, tokenizer)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Explain the model's predictions\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrev1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrev2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Visualize SHAP explanation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values, tokenizer\u001b[38;5;241m.\u001b[39mvocab, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ali-ml/lib/python3.10/site-packages/shap/explainers/_partition.py:128\u001b[0m, in \u001b[0;36mPartition.__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, fixed_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    125\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ali-ml/lib/python3.10/site-packages/shap/explainers/_explainer.py:264\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 264\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    269\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/envs/ali-ml/lib/python3.10/site-packages/shap/explainers/_partition.py:153\u001b[0m, in \u001b[0;36mPartition.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# if not fixed background or no base value assigned then compute base value for a row\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_base_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_background\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_base_value \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm00\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# the zero index param tells the masked model what the baseline is\u001b[39;00m\n\u001b[1;32m    154\u001b[0m f11 \u001b[38;5;241m=\u001b[39m fm(\u001b[38;5;241m~\u001b[39mm00\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker\u001b[38;5;241m.\u001b[39mclustering):\n",
      "File \u001b[0;32m~/anaconda3/envs/ali-ml/lib/python3.10/site-packages/shap/utils/_masked_model.py:69\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_masking_call(full_masks, zero_index\u001b[38;5;241m=\u001b[39mzero_index, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ali-ml/lib/python3.10/site-packages/shap/utils/_masked_model.py:147\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    145\u001b[0m     joined_masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([np\u001b[38;5;241m.\u001b[39mconcatenate(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_masked_inputs])\n\u001b[1;32m    146\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mjoined_masked_inputs)\n\u001b[0;32m--> 147\u001b[0m     \u001b[43m_assert_output_input_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoined_masked_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n\u001b[1;32m    149\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ali-ml/lib/python3.10/site-packages/shap/utils/_masked_model.py:270\u001b[0m, in \u001b[0;36m_assert_output_input_match\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_output_input_match\u001b[39m(inputs, outputs):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m]), \\\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model produced \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(outputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output rows when given \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input rows! Check the implementation of the model you provided for errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model produced 2 output rows when given 1 input rows! Check the implementation of the model you provided for errors."
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Define a function that computes similarity using cosine similarity\n",
    "def predict_similarity(texts):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #embeddings = [model(**tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)).last_hidden_state[0][0] for text in texts]\n",
    "    similarity = cosine_similarity(review1_embedding[0].reshape(1, -1), review2_embedding[1].reshape(1, -1))[0][0]\n",
    "    return [1 - similarity, similarity]\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(predict_similarity, tokenizer)\n",
    "\n",
    "# Explain the model's predictions\n",
    "shap_values = explainer([rev1, rev2])\n",
    "\n",
    "# Visualize SHAP explanation\n",
    "shap.summary_plot(shap_values, tokenizer.vocab, show=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57c58b82-ec73-444f-8c08-8a9fb927ffbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf719822-54c3-4274-bcd9-c612cb5d6295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4063a9c-e347-4815-bccb-bcd9a1736bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
